{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.errors import HttpError\n",
    "\n",
    "# # If modifying these scopes, delete the file token.json.\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\",\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "\n",
    "# creds = None\n",
    "# # The file token.json stores the user's access and refresh tokens, and is\n",
    "# # created automatically when the authorization flow completes for the first\n",
    "# # time.\n",
    "# if os.path.exists(\"token.json\"):\n",
    "#   creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# # If there are no (valid) credentials available, let the user log in.\n",
    "# if not creds or not creds.valid:\n",
    "#   if creds and creds.expired and creds.refresh_token:\n",
    "#     creds.refresh(Request())\n",
    "#   else:\n",
    "#     flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#         \"credentials.json\", SCOPES\n",
    "#     )\n",
    "#     creds = flow.run_local_server(port=0)\n",
    "#   # Save the credentials for the next run\n",
    "#   with open(\"token.json\", \"w\") as token:\n",
    "#     token.write(creds.to_json())\n",
    "\n",
    "# try:\n",
    "#   youtube = build(\"youtube\", \"v3\", credentials=creds)\n",
    "\n",
    "#       # Taking input from the user and slicing for video id\n",
    "#   video_id = input('Enter Youtube Video URL: ')[-11:]\n",
    "#   print(\"video id: \" + video_id)\n",
    "  \n",
    "#   # Getting the channelId of the video uploader\n",
    "#   video_response = youtube.videos().list(\n",
    "#       part='snippet',\n",
    "#       id=video_id\n",
    "#   ).execute()\n",
    "  \n",
    "#   print(video_response)\n",
    "#   # Splitting the response for channelID\n",
    "#   video_snippet = video_response['items'][0]['snippet']\n",
    "#   uploader_channel_id = video_snippet['channelId']\n",
    "#   print(\"channel id: \" + uploader_channel_id)\n",
    "  \n",
    "# except HttpError as err:\n",
    "#   print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Put in your API Key\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m youtube \u001b[38;5;241m=\u001b[39m \u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myoutube\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mv3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeveloperKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# initializing Youtube API\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Improved input handling for video ID\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_video_id\u001b[39m(url):\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/googleapiclient/discovery.py:315\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(serviceName, version, http, discoveryServiceUrl, developerKey, model, requestBuilder, credentials, cache_discovery, cache, client_options, adc_cert_path, adc_key_path, num_retries, static_discovery, always_use_jwt_access)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     content \u001b[38;5;241m=\u001b[39m _retrieve_discovery_doc(\n\u001b[1;32m    305\u001b[0m         requested_url,\n\u001b[1;32m    306\u001b[0m         discovery_http,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         static_discovery\u001b[38;5;241m=\u001b[39mstatic_discovery,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     service \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_from_document\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscovery_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeveloperKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeveloperKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequestBuilder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequestBuilder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43madc_cert_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madc_cert_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43madc_key_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madc_key_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# exit if a service was created\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HttpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/googleapiclient/discovery.py:616\u001b[0m, in \u001b[0;36mbuild_from_document\u001b[0;34m(service, base, future, http, developerKey, model, requestBuilder, credentials, client_options, adc_cert_path, adc_key_path, always_use_jwt_access)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# If the user didn't pass in credentials, attempt to acquire application\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# default credentials.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 616\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m \u001b[43m_auth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_credentials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Check google-api-core >= 2.18.0 if credentials' universe != \"googleapis.com\".\u001b[39;00m\n\u001b[1;32m    622\u001b[0m _check_api_core_compatible_with_credentials_universe(credentials)\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/googleapiclient/_auth.py:57\u001b[0m, in \u001b[0;36mdefault_credentials\u001b[0;34m(scopes, quota_project_id)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns Application Default Credentials.\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HAS_GOOGLE_AUTH:\n\u001b[0;32m---> 57\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m credentials\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m HAS_OAUTH2CLIENT:\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/google/auth/_default.py:693\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    685\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    689\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[1;32m    690\u001b[0m             )\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "API_KEY = ''  # Put in your API Key\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)  # initializing Youtube API\n",
    "\n",
    "# Improved input handling for video ID\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "video_url = input('Enter YouTube Video URL: ')\n",
    "video_id = extract_video_id(video_url)\n",
    "\n",
    "if not video_id:\n",
    "    print(\"Invalid YouTube URL\")\n",
    "else:\n",
    "    print(\"video id: \" + video_id)\n",
    "\n",
    "    # Getting the channelId of the video uploader\n",
    "    try:\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        print(\"Video Response:\", video_response)  # Debugging step\n",
    "\n",
    "        if 'items' in video_response and video_response['items']:\n",
    "            video_snippet = video_response['items'][0]['snippet']\n",
    "            uploader_channel_id = video_snippet.get('channelId', 'No channelId found')\n",
    "            print(\"channel id: \" + uploader_channel_id)\n",
    "\n",
    "            # Fetch comments\n",
    "            print(\"Fetching Comments...\")\n",
    "            comments = []\n",
    "            nextPageToken = None\n",
    "            while len(comments) < 600:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100,  # You can fetch up to 100 comments per request\n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    # Check if the comment is not from the video uploader\n",
    "                    if comment['authorChannelId']['value'] != uploader_channel_id:\n",
    "                        comments.append(comment['textDisplay'])\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "            # Print the first 5 comments\n",
    "            print(comments[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"No video found for the provided ID.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel to CsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "import time\n",
    "\n",
    "# Get the channel name and upload playlist ID\n",
    "def get_channel_info(channel_id):\n",
    "    response = youtube.channels().list(\n",
    "        part='snippet,contentDetails',\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        channel = response['items'][0]['snippet']['title']\n",
    "        upload_playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        return channel, upload_playlist_id\n",
    "    return None, None\n",
    "\n",
    "# Fetch 100 video IDs from the uploads playlist\n",
    "def get_video_ids_from_channel(upload_playlist_id, max_results=100):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        response = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=upload_playlist_id,\n",
    "            maxResults=50,  # API allows max 50 results per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['snippet']['resourceId']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(video_ids) >= max_results:\n",
    "            break\n",
    "    \n",
    "    return video_ids[:max_results]\n",
    "\n",
    "# Fetch the video title\n",
    "def get_video_title(video_id):\n",
    "    response = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        return response['items'][0]['snippet']['title']\n",
    "    return None\n",
    "\n",
    "# Fetch comments for a video\n",
    "def get_comments(video_id, max_comments=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Fetch up to 100 comments per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(comments) >= max_comments:\n",
    "            break\n",
    "\n",
    "    return comments[:max_comments]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "def write_to_csv(channel_name, video_data):\n",
    "    with open(f'{channel_name}_video_comments.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Channel Name\", \"Video Title\", \"Comment\"])\n",
    "        \n",
    "        for video_title, comments in video_data.items():\n",
    "            for comment in comments:\n",
    "                writer.writerow([channel_name, video_title, comment])\n",
    "\n",
    "# Main Function\n",
    "def fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100):\n",
    "    start_time = time.time()\n",
    "    # Get channel information\n",
    "    channel_name, upload_playlist_id = get_channel_info(channel_id)\n",
    "    \n",
    "    if not channel_name or not upload_playlist_id:\n",
    "        print(\"Invalid channel ID or channel information not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Fetching data for channel: {channel_name}\")\n",
    "\n",
    "    # Get video IDs\n",
    "    video_ids = get_video_ids_from_channel(upload_playlist_id, max_results=max_videos)\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # For each video, get the title and comments\n",
    "    for video_id in video_ids:\n",
    "        video_title = get_video_title(video_id)\n",
    "        if video_title:\n",
    "            print(f\"Fetching comments for video: {video_title}\")\n",
    "            comments = get_comments(video_id, max_comments=max_comments_per_video)\n",
    "            video_data[video_title] = comments\n",
    "    \n",
    "    \n",
    "    # Write all data to a CSV\n",
    "    write_to_csv(channel_name, video_data)\n",
    "    print(f\"Data saved to {channel_name}_video_comments.csv\")\n",
    "    end_time = time.time()\n",
    "    execution_time = start_time - end_time\n",
    "    print(\"Execution time:\",execution_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for channel: MrBeast\n",
      "Fetching comments for video: How Many Twins Can You Spot?\n",
      "Fetching comments for video: Spot The Difference, Win $10,000\n",
      "Fetching comments for video: 100 Identical Twins Fight For $250,000\n",
      "Fetching comments for video: Running With Bigger And Bigger Lunchlys\n",
      "Fetching comments for video: Holding Bigger And Bigger Dogs\n",
      "Fetching comments for video: Men Vs Women Survive The Wilderness For $500,000\n",
      "Fetching comments for video: Will A Guitar Boat Hold My Weight?\n",
      "Fetching comments for video: Real Or Cake For $10,000\n",
      "Fetching comments for video: 7 Days Stranded In A Cave\n",
      "Fetching comments for video: Running With Bigger And Bigger Feastables\n",
      "Fetching comments for video: Survive 100 Days In Nuclear Bunker, Win $500,000\n",
      "Fetching comments for video: Spot The Fake Animal For $10,000\n",
      "Fetching comments for video: 50 YouTubers Fight For $1,000,000\n",
      "Fetching comments for video: Pass The Phone To‚Ä¶\n",
      "Fetching comments for video: How Many Balloons Does It Take To Fly?\n",
      "Fetching comments for video: I Built 100 Houses And Gave Them Away!\n",
      "Fetching comments for video: World‚Äôs Deadliest Obstacle Course!\n",
      "Fetching comments for video: $10,000 Every Day You Survive In The Wilderness\n",
      "Fetching comments for video: Sprinting with More and More Money\n",
      "Fetching comments for video: Giving 1000 Phones Away\n",
      "Fetching comments for video: Bottle Head Smashing World Record Attempt!\n",
      "Fetching comments for video: Protect The Yacht, Keep It!\n",
      "Fetching comments for video: Ages 1-100 Try My Chocolate\n",
      "Fetching comments for video: Spot The Hidden People For $10,000\n",
      "Fetching comments for video: Would You Split Or Steal $250,000?\n",
      "Fetching comments for video: Buy Feastables, Win Unlimited Money\n",
      "Fetching comments for video: In 10 Minutes This Room Will Explode!\n",
      "Fetching comments for video: The World's Fastest Cleaners\n",
      "Fetching comments for video: Ages 1 - 100 Decide Who Wins $250,000\n",
      "Fetching comments for video: Guess The Gift, Keep It\n",
      "Fetching comments for video: I‚Äôm Giving My 250M Subscriber $25,000\n",
      "Fetching comments for video: Anything You Touch, You Keep!\n",
      "Fetching comments for video: 7 Days Stranded On An Island\n",
      "Fetching comments for video: Keep Track Of Car, Win $10,000\n",
      "Fetching comments for video: Protect The Lamborghini, Keep It!\n",
      "Fetching comments for video: I Survived 7 Days In An Abandoned City\n",
      "Fetching comments for video: Unboxing My 200M Subscriber Play Button\n",
      "Fetching comments for video: I Filled Chandler‚Äôs Car With Feastables\n",
      "Fetching comments for video: Buy Feastables, Win $10,000\n",
      "Fetching comments for video: Face Your Biggest Fear To Win $800,000\n",
      "Fetching comments for video: $1 vs $250,000,000 Private Island!\n",
      "Fetching comments for video: Protect $500,000 Keep It!\n",
      "Fetching comments for video: I Spent 7 Days In Solitary Confinement\n",
      "Fetching comments for video: I Saved 100 Dogs From Dying\n",
      "Fetching comments for video: Survive 100 Days Trapped, Win $500,000\n",
      "Fetching comments for video: Feeding A Dog $1 vs $10,000 Steak\n",
      "Fetching comments for video: Could You Walk Up A Skyscraper?\n",
      "Fetching comments for video: $10,000 Every Day You Survive In A Grocery Store\n",
      "Fetching comments for video: $1 vs $10,000,000 Job!\n",
      "Fetching comments for video: I Spent 7 Days Buried Alive\n",
      "Fetching comments for video: I Gave Away A House On Halloween\n",
      "Fetching comments for video: Giving Car Keys Instead Of Candy On Halloween\n",
      "Fetching comments for video: I Built 100 Wells In Africa\n",
      "Fetching comments for video: Furthest Away From Me Wins $10,000\n",
      "Fetching comments for video: World‚Äôs Most Expensive Bed\n",
      "Fetching comments for video: World‚Äôs Deadliest Laser Maze!\n",
      "Fetching comments for video: World‚Äôs Most Expensive Coffee\n",
      "Fetching comments for video: $100,000,000 Bathroom\n",
      "Fetching comments for video: Miranda Cosgrove Said What?\n",
      "Fetching comments for video: $1 vs $100,000,000 House!\n",
      "Fetching comments for video: I Tipped A Pizza Delivery Driver A Car\n",
      "Fetching comments for video: World's Most Dangerous Trap!\n",
      "Fetching comments for video: I NEED 1 MORE ùó¶ùó®ùóïùó¶ùóñùó•ùóúùóïùóòùó•\n",
      "Fetching comments for video: Guess The Gift, Keep It\n",
      "Fetching comments for video: Spot The Hidden People For $10,000\n",
      "Fetching comments for video: I Paid A Random Student‚Äôs College Tuition\n",
      "Fetching comments for video: $100,000,000 Car Doors\n",
      "Fetching comments for video: $1 vs $100,000,000 Car!\n",
      "Fetching comments for video: Extreme Home Makeover!\n",
      "Fetching comments for video: Katana Vs Bullet\n",
      "Fetching comments for video: How Many School Buses Can We Stack?\n",
      "Fetching comments for video: Lamborghini Vs World's Largest Shredder\n",
      "Fetching comments for video: Feeding A Cat $10 Vs $10,000 Sushi\n",
      "Fetching comments for video: Make This Kick, Win Super Bowl Tickets\n",
      "Fetching comments for video: Metal Pipe Vs School Bus\n",
      "Fetching comments for video: Every Country On Earth Fights For $250,000!\n",
      "Fetching comments for video: Can You Beat A Girl In Arm Wrestling?\n",
      "Fetching comments for video: $1 vs $250,000 Vacation!\n",
      "Fetching comments for video: I Ate The World‚Äôs Most Poisonous Fish\n",
      "Fetching comments for video: 7 Days Stranded At Sea\n",
      "Fetching comments for video: I Traded My Car At a Red Light\n",
      "Fetching comments for video: Train Vs Giant Pit\n",
      "Fetching comments for video: I Buried Treasure in the Bermuda Triangle\n",
      "Fetching comments for video: $1 vs $1,000,000,000 Yacht!\n",
      "Fetching comments for video: Do Pawnshops Scam You?\n",
      "Fetching comments for video: Ages 1 - 100 Fight For $500,000\n",
      "Fetching comments for video: I Got Naruto to Subscribe to Me\n",
      "Fetching comments for video: I Made An Egg Sandwich With @BayashiTV_\n",
      "Fetching comments for video: 1,000 Deaf People Hear For The First Time\n",
      "Fetching comments for video: Would You Pet a Cheetah in Africa?\n",
      "Fetching comments for video: Tipping A Waitress A Car\n",
      "Fetching comments for video: Would you go on a Blind Date in Italy?\n",
      "Fetching comments for video: $1 vs $500,000 Plane Ticket!\n",
      "Fetching comments for video: I Sent a Subscriber to Disneyland\n",
      "Fetching comments for video: I Paid A Real Assassin To Try To Kill Me\n",
      "Fetching comments for video: Do Men Lie About Their Height?\n",
      "Fetching comments for video: 1,000 Blind People See For The First Time\n",
      "Fetching comments for video: I Survived 50 Hours In Antarctica\n",
      "Fetching comments for video: Hydraulic Press Vs Lamborghini\n",
      "Fetching comments for video: Would You Fly To Paris For A Baguette?\n",
      "Data saved to MrBeast_video_comments.csv\n",
      "Execution time: -26.033523082733154\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Replace with the actual YouTube channel ID\n",
    "channel_id = f'UCX6OQ3DkcsbYNE6H8uQQuVA'\n",
    "fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Processing for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"GPU is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                              Video Title  \\\n",
      "0      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "1      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "2      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "3      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "4      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "\n",
      "                                  Comment  \n",
      "0                              I love you  \n",
      "1  Please give me one iPhoneüò¢üò¢üò¢üò¢!ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫  \n",
      "2       ‚ù§‚ù§üòÇüòÇüéâüò¢üòÆüòÆüòÖüòäüòä‚ù§‚ù§‚ù§üòÇüòÇüòÇüéâüéâüò¢üò¢üò¢üòÆüòÆüòÖüòÖüòäüòä‚ù§‚ù§üòÇüòÇüò¢  \n",
      "3        It&#39;s a lot it&#39;s all good  \n",
      "4         Can you buy a car for my dad ??  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Channel Name  9800 non-null   object\n",
      " 1   Video Title   9800 non-null   object\n",
      " 2   Comment       9800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 229.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Preprocess the data\n",
    "# Use the 'Comment' column as our input and 'Video Title' as our target\n",
    "X = df['Comment'].values\n",
    "y = df['Video Title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 processed comments with emojis:\n",
      "0                      i love you\n",
      "1       please give me one iphone\n",
      "2                                \n",
      "3          its a lot its all good\n",
      "4    can you buy a car for my dad\n",
      "Name: processed_comment_with_emojis, dtype: object\n",
      "                                  Comment  \\\n",
      "0                              I love you   \n",
      "1  Please give me one iPhoneüò¢üò¢üò¢üò¢!ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫   \n",
      "2       ‚ù§‚ù§üòÇüòÇüéâüò¢üòÆüòÆüòÖüòäüòä‚ù§‚ù§‚ù§üòÇüòÇüòÇüéâüéâüò¢üò¢üò¢üòÆüòÆüòÖüòÖüòäüòä‚ù§‚ù§üòÇüòÇüò¢   \n",
      "3        It&#39;s a lot it&#39;s all good   \n",
      "4         Can you buy a car for my dad ??   \n",
      "\n",
      "                                Preprocessed_Comment  \n",
      "0                                         I love you  \n",
      "1  Please give me one iPhone:crying_face::crying_...  \n",
      "2  :red_heart::red_heart::face_with_tears_of_joy:...  \n",
      "3                   It&#39;s a lot it&#39;s all good  \n",
      "4                    Can you buy a car for my dad ??  \n",
      "First comment tensor:\n",
      "tensor([ 73,  32, 108, 111, 118, 101,  32, 121, 111, 117])\n",
      "Shape of the first tensor: torch.Size([10])\n",
      "Total number of tensors: 9800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from emoji import demojize\n",
    "\n",
    "\n",
    "# Update the preprocessing function to include emoji conversion\n",
    "def preprocess_text_with_emojis(text):\n",
    "    # Convert emojis to text\n",
    "    \n",
    "    emoji_pattern = re.compile(r':([a-zA-Z_]+):')\n",
    "    unique_emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # Reconstruct the text with only one instance of each emoji\n",
    "    for emote in unique_emojis:\n",
    "        text = re.sub(f'(:{emote}:)+', f':{emote}:', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Reprocess the comments with the updated function\n",
    "df['processed_comment_with_emojis'] = df['Comment'].apply(preprocess_text_with_emojis)\n",
    "\n",
    "print(\"\\\n",
    "First 5 processed comments with emojis:\")\n",
    "\n",
    "print(df['processed_comment_with_emojis'].head())\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Replace emojis with their meanings\n",
    "    return demojize(comment, language='en')\n",
    "\n",
    "# Preprocess the comments\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(df[['Comment', 'Preprocessed_Comment']].head())\n",
    "\n",
    "# Convert preprocessed comments to PyTorch tensors\n",
    "comment_tensors = [torch.tensor([ord(c) for c in comment]) for comment in df['Preprocessed_Comment']]\n",
    "\n",
    "# Display the first tensor\n",
    "print(\"\\\n",
    "First comment tensor:\")\n",
    "print(comment_tensors[0])\n",
    "\n",
    "# Print the shape of the first tensor\n",
    "print(\"\\\n",
    "Shape of the first tensor:\", comment_tensors[0].shape)\n",
    "\n",
    "# Print the total number of tensors\n",
    "print(\"\\\n",
    "Total number of tensors:\", len(comment_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Comment Sentiment\n",
      "0                       ŸÖŸÖŸÜŸàŸÜ ÿ®ÿßÿ®ÿ™ ÿ≤€åÿ±ŸÜŸà€åÿ≥ ŸÅÿßÿ±ÿ≥€å‚ù§Ô∏èüáÆüá∑  Positive\n",
      "1                                              üéâüéâüéâüéâüòä   Neutral\n",
      "2  There is 200. Because in the video it&#39;s 10...   Neutral\n",
      "3                                       Looking epic   Neutral\n",
      "4                                   love Mr.beast üá≤üá®   Neutral\n",
      "5  &quot;let&#39;s play basketball but with dumbb...   Neutral\n",
      "6                                Very good marketing   Neutral\n",
      "7                                           Ty jimmy   Neutral\n",
      "8                                        A HUNDRED üéâ   Neutral\n",
      "9                       ive seen this video in my tv   Neutral\n",
      "Sentiment Distribution:\n",
      "Sentiment\n",
      "Neutral     0.795102\n",
      "Positive    0.167755\n",
      "Negative    0.037143\n",
      "Name: proportion, dtype: float64\n",
      "Labeled data saved to 'MrBeast_video_comments_labeled.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from emoji import demojize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def simple_sentiment_analysis(text):\n",
    "    text = demojize(text.lower())\n",
    "    \n",
    "    positive_words = [ ':smiling_face:', ':red_heart:']\n",
    "    negative_words = [ ':angry_face:', ':crying_face:']\n",
    "    \n",
    "    positive_count = sum(1 for word in positive_words if word in text)\n",
    "    negative_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif negative_count > positive_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Apply sentiment analysis to all comments\n",
    "df['Sentiment'] = df['Comment'].apply(simple_sentiment_analysis)\n",
    "\n",
    "# Display the first few rows with sentiments\n",
    "print(df[['Comment', 'Sentiment']].head(10))\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\\n",
    "Sentiment Distribution:\")\n",
    "print(df['Sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Save the labeled data\n",
    "df.to_csv('MrBeast_video_comments_labeled.csv', index=False)\n",
    "print(\"\\\n",
    "Labeled data saved to 'MrBeast_video_comments_labeled.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                   Video Title  \\\n",
      "0      MrBeast  How Many Twins Can You Spot?   \n",
      "1      MrBeast  How Many Twins Can You Spot?   \n",
      "2      MrBeast  How Many Twins Can You Spot?   \n",
      "3      MrBeast  How Many Twins Can You Spot?   \n",
      "4      MrBeast  How Many Twins Can You Spot?   \n",
      "\n",
      "                                             Comment  \n",
      "0                       ŸÖŸÖŸÜŸàŸÜ ÿ®ÿßÿ®ÿ™ ÿ≤€åÿ±ŸÜŸà€åÿ≥ ŸÅÿßÿ±ÿ≥€å‚ù§Ô∏èüáÆüá∑  \n",
      "1                                              üéâüéâüéâüéâüòä  \n",
      "2  There is 200. Because in the video it&#39;s 10...  \n",
      "3                                       Looking epic  \n",
      "4                                   love Mr.beast üá≤üá®  \n",
      "Total number of comments: 9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading vader_lexicon: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/shreyas/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/share/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124mTotal number of comments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize the VADER sentiment analyzer\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m sid \u001b[38;5;241m=\u001b[39m \u001b[43mSentimentIntensityAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Function to get sentiment label based on compound score\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_sentiment_label\u001b[39m(compound_score):\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/nltk/sentiment/vader.py:340\u001b[0m, in \u001b[0;36mSentimentIntensityAnalyzer.__init__\u001b[0;34m(self, lexicon_file)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    338\u001b[0m     lexicon_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m ):\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon_file \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexicon_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlexicon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_lex_dict()\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstants \u001b[38;5;241m=\u001b[39m VaderConstants()\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/nltk/data.py:836\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 836\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m \u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresource_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    839\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/nltk/data.py:962\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    959\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen()\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/nltk/data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - '/Users/shreyas/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/share/nltk_data'\n    - '/Users/shreyas/YoutubeNLP/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# Download the VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(f\"\\\n",
    "Total number of comments: {len(df)}\")\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment label based on compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply sentiment analysis to all comments\n",
    "df['Sentiment_Score'] = df['Comment'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
    "df['Sentiment'] = df['Sentiment_Score'].apply(get_sentiment_label)\n",
    "\n",
    "# Display the first few rows with sentiments\n",
    "print(df[['Comment', 'Sentiment_Score', 'Sentiment']].head(10))\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\\n",
    "Sentiment Distribution:\")\n",
    "print(df['Sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Save the labeled data\n",
    "output_file = 'MrBeast_video_comments_labeled_vader.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\\n",
    "Labeled data saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'MrBeast_video_comments_labeled_vader.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMrBeast_video_comments_labeled_vader.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Define sentiment values for specific emojis\u001b[39;00m\n\u001b[1;32m      9\u001b[0m emoji_sentiment \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:red_heart:\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:smiling_face_with_smiling_eyes:\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:thumbs_down:\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m     17\u001b[0m }\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/YoutubeNLP/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MrBeast_video_comments_labeled_vader.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from emoji import demojize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('MrBeast_video_comments_labeled_vader.csv')\n",
    "\n",
    "# Define sentiment values for specific emojis\n",
    "emoji_sentiment = {\n",
    "    ':red_heart:': 0.5,\n",
    "    ':smiling_face_with_smiling_eyes:': 0.2,\n",
    "    ':face_with_tears_of_joy:': 0.4,\n",
    "    ':thumbs_up:': 0.2,\n",
    "    ':crying_face:': -0.1,\n",
    "    ':angry_face:': -0.3,\n",
    "    ':thumbs_down:': -0.4\n",
    "}\n",
    "\n",
    "# Function to calculate emoji sentiment score\n",
    "def calculate_emoji_sentiment(text):\n",
    "    text = demojize(text)\n",
    "    score = 0\n",
    "    for emoji, value in emoji_sentiment.items():\n",
    "        score += text.count(emoji) * value\n",
    "    return score\n",
    "\n",
    "# Function to get sentiment label based on compound score\n",
    "def get_sentiment_label(compound_score):\n",
    "    if compound_score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif compound_score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Update sentiment scores with emoji sentiment\n",
    "df['Emoji_Sentiment_Score'] = df['Comment'].apply(calculate_emoji_sentiment)\n",
    "df['Updated_Sentiment_Score'] = df['Sentiment_Score'] + df['Emoji_Sentiment_Score']\n",
    "\n",
    "# Reclassify sentiments based on updated scores\n",
    "df['Updated_Sentiment'] = df['Updated_Sentiment_Score'].apply(get_sentiment_label)\n",
    "\n",
    "# Display the first few rows with updated sentiments\n",
    "print(df[['Comment', 'Sentiment_Score', 'Emoji_Sentiment_Score', 'Updated_Sentiment_Score', 'Updated_Sentiment']].head(10))\n",
    "\n",
    "# Display updated sentiment distribution\n",
    "print(\"\\\n",
    "Updated Sentiment Distribution:\")\n",
    "print(df['Updated_Sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Save the updated labeled data\n",
    "output_file = 'MrBeast_video_comments_labeled_vader_with_emoji.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\\n",
    "Updated labeled data saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
