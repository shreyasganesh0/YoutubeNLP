{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.errors import HttpError\n",
    "\n",
    "# # If modifying these scopes, delete the file token.json.\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\",\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "\n",
    "# creds = None\n",
    "# # The file token.json stores the user's access and refresh tokens, and is\n",
    "# # created automatically when the authorization flow completes for the first\n",
    "# # time.\n",
    "# if os.path.exists(\"token.json\"):\n",
    "#   creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# # If there are no (valid) credentials available, let the user log in.\n",
    "# if not creds or not creds.valid:\n",
    "#   if creds and creds.expired and creds.refresh_token:\n",
    "#     creds.refresh(Request())\n",
    "#   else:\n",
    "#     flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#         \"credentials.json\", SCOPES\n",
    "#     )\n",
    "#     creds = flow.run_local_server(port=0)\n",
    "#   # Save the credentials for the next run\n",
    "#   with open(\"token.json\", \"w\") as token:\n",
    "#     token.write(creds.to_json())\n",
    "\n",
    "# try:\n",
    "#   youtube = build(\"youtube\", \"v3\", credentials=creds)\n",
    "\n",
    "#       # Taking input from the user and slicing for video id\n",
    "#   video_id = input('Enter Youtube Video URL: ')[-11:]\n",
    "#   print(\"video id: \" + video_id)\n",
    "  \n",
    "#   # Getting the channelId of the video uploader\n",
    "#   video_response = youtube.videos().list(\n",
    "#       part='snippet',\n",
    "#       id=video_id\n",
    "#   ).execute()\n",
    "  \n",
    "#   print(video_response)\n",
    "#   # Splitting the response for channelID\n",
    "#   video_snippet = video_response['items'][0]['snippet']\n",
    "#   uploader_channel_id = video_snippet['channelId']\n",
    "#   print(\"channel id: \" + uploader_channel_id)\n",
    "  \n",
    "# except HttpError as err:\n",
    "#   print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video id: oWo9SNcyxlI\n",
      "Video Response: {'kind': 'youtube#videoListResponse', 'etag': 'FDRV--UbntAuYJOaIEc1BeeHFCo', 'items': [{'kind': 'youtube#video', 'etag': 'ppDsgJg80plQ9aIWXXT-RcV1qtM', 'id': 'oWo9SNcyxlI', 'snippet': {'publishedAt': '2022-07-27T13:15:20Z', 'channelId': 'UC8UmuMDlWB1egqH5nyy0pDw', 'title': 'Sentiment Analysis with LSTM | Deep Learning with Keras | Neural Networks | Project#8', 'description': 'We shall train three separate Neural Networks, namely: a Simple Neural Net, a Convolutional Neural Net and a Long Short Term Memory Neural Net. LSTM networks are actually considered to be quite suitable for handling NLP problems, and by the end of this video, you will understand why.\\n\\nSentiment Classification LSTM Model we shall be training as part of this tutorial is so good that it not only predicts the user movie review sentiment as positive/negative, it also does a fabulous job in predicting IMDb Rating itself corresponding to these reviews with mind blowing accuracy.\\n\\n🔥 During the course of next ~25mins, we shall discuss:\\na. Word Embedding (which is a powerful feature extraction technique), \\nb. Intuition on various Neural Network approaches (Simple / CNN /LSTM), \\nc. Concept of Overfitting (that happens when a model memorises training data, rather than understanding general distribution) \\nd. And, of course, our high-level solution architecture for training our Sentiment Classification Models\\n\\n🔥 Sections:\\n\\n00:00 Introduction\\n02:17 Solution Architecture\\n03:50 Word Embeddings, Neural Networks & Overfitting\\n08:56 Free Skillcate Project Toolkit\\n09:55 Code: Intro\\n11:42 Code: Data Preprocessing\\n13:44 Code: Tokenizer & Embedding Layer\\n15:50 Code: Simple NN Training\\n17:50 Code: CNN Training\\n19:27 Code: LSTM Training\\n21:03 Having fun with Model\\n22:59 Updates on Free 1:1 Mentoring\\n\\n🔥 Important Links:\\n\\nToolkit link: https://drive.google.com/drive/folders/1TK9k41RT8Nf3IhzerNWHpEqWztsk2gAP?usp=sharing\\n\\nSentiment Analysis Project based Review Classification Project from Skillcate: https://youtu.be/zwR6M5zpnWs\\n\\nSentiment Analysis Project (End-to-end) with ML Model Building + Deployment (using Flask):\\n- Model Building: https://youtu.be/lKAdxN0qrgk (Part-1)\\n- Model Deployment: https://youtu.be/KEQCVwJU5KU (Part-2)\\n\\nGitHub Repo Link: https://github.com/skillcate/movie-sentiment-analysis-with-deep-neural-networks.git\\n\\n\\n🔥 Do like, share & subscribe to our channel. Keep in touch:\\n\\nEmail: skillcate@gmail.com\\nWebsite: https://www.skillcate.com\\nFacebook: https://www.facebook.com/groups/mlprojects\\nTelegram: https://t.me/skillcate', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/hqdefault.jpg', 'width': 480, 'height': 360}, 'standard': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/sddefault.jpg', 'width': 640, 'height': 480}, 'maxres': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/maxresdefault.jpg', 'width': 1280, 'height': 720}}, 'channelTitle': 'Skillcate AI', 'tags': ['sentiment analysis', 'sentiment analysis project', 'sentiment analysis with deep learning', 'sentiment analysis lstm', 'sentiment analysis using machine learning', 'sentiment analysis using lstm', 'sentiment analysis with lstm', 'lstm application', 'sentiment analysis with keras', 'sentiment analysis deep learning', 'amazon alexa review classification', 'review classification', 'review classification using deep learning', 'deep learning project', 'lstm project', 'sentiment analysis neural network'], 'categoryId': '27', 'liveBroadcastContent': 'none', 'defaultLanguage': 'en-IN', 'localized': {'title': 'Sentiment Analysis with LSTM | Deep Learning with Keras | Neural Networks | Project#8', 'description': 'We shall train three separate Neural Networks, namely: a Simple Neural Net, a Convolutional Neural Net and a Long Short Term Memory Neural Net. LSTM networks are actually considered to be quite suitable for handling NLP problems, and by the end of this video, you will understand why.\\n\\nSentiment Classification LSTM Model we shall be training as part of this tutorial is so good that it not only predicts the user movie review sentiment as positive/negative, it also does a fabulous job in predicting IMDb Rating itself corresponding to these reviews with mind blowing accuracy.\\n\\n🔥 During the course of next ~25mins, we shall discuss:\\na. Word Embedding (which is a powerful feature extraction technique), \\nb. Intuition on various Neural Network approaches (Simple / CNN /LSTM), \\nc. Concept of Overfitting (that happens when a model memorises training data, rather than understanding general distribution) \\nd. And, of course, our high-level solution architecture for training our Sentiment Classification Models\\n\\n🔥 Sections:\\n\\n00:00 Introduction\\n02:17 Solution Architecture\\n03:50 Word Embeddings, Neural Networks & Overfitting\\n08:56 Free Skillcate Project Toolkit\\n09:55 Code: Intro\\n11:42 Code: Data Preprocessing\\n13:44 Code: Tokenizer & Embedding Layer\\n15:50 Code: Simple NN Training\\n17:50 Code: CNN Training\\n19:27 Code: LSTM Training\\n21:03 Having fun with Model\\n22:59 Updates on Free 1:1 Mentoring\\n\\n🔥 Important Links:\\n\\nToolkit link: https://drive.google.com/drive/folders/1TK9k41RT8Nf3IhzerNWHpEqWztsk2gAP?usp=sharing\\n\\nSentiment Analysis Project based Review Classification Project from Skillcate: https://youtu.be/zwR6M5zpnWs\\n\\nSentiment Analysis Project (End-to-end) with ML Model Building + Deployment (using Flask):\\n- Model Building: https://youtu.be/lKAdxN0qrgk (Part-1)\\n- Model Deployment: https://youtu.be/KEQCVwJU5KU (Part-2)\\n\\nGitHub Repo Link: https://github.com/skillcate/movie-sentiment-analysis-with-deep-neural-networks.git\\n\\n\\n🔥 Do like, share & subscribe to our channel. Keep in touch:\\n\\nEmail: skillcate@gmail.com\\nWebsite: https://www.skillcate.com\\nFacebook: https://www.facebook.com/groups/mlprojects\\nTelegram: https://t.me/skillcate'}, 'defaultAudioLanguage': 'en-IN'}}], 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}\n",
      "channel id: UC8UmuMDlWB1egqH5nyy0pDw\n",
      "Fetching Comments...\n",
      "['can we stack these 3 models on top of each other <br>if so how?', 'Bro, you missed the padding function(pad_sequences) to include in code....', 'Can we use same procedure for emotions instead of sentiments ....and for product reviews', 'Long live your hand on the explanation', 'can you tell how to add flask to it please']\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "API_KEY = 'AIzaSyAr-ZhrgZaE_jz_jXT18UuJJH7Z-H43gug'  # Put in your API Key\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)  # initializing Youtube API\n",
    "\n",
    "# Improved input handling for video ID\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "video_url = input('Enter YouTube Video URL: ')\n",
    "video_id = extract_video_id(video_url)\n",
    "\n",
    "if not video_id:\n",
    "    print(\"Invalid YouTube URL\")\n",
    "else:\n",
    "    print(\"video id: \" + video_id)\n",
    "\n",
    "    # Getting the channelId of the video uploader\n",
    "    try:\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        print(\"Video Response:\", video_response)  # Debugging step\n",
    "\n",
    "        if 'items' in video_response and video_response['items']:\n",
    "            video_snippet = video_response['items'][0]['snippet']\n",
    "            uploader_channel_id = video_snippet.get('channelId', 'No channelId found')\n",
    "            print(\"channel id: \" + uploader_channel_id)\n",
    "\n",
    "            # Fetch comments\n",
    "            print(\"Fetching Comments...\")\n",
    "            comments = []\n",
    "            nextPageToken = None\n",
    "            while len(comments) < 600:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100,  # You can fetch up to 100 comments per request\n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    # Check if the comment is not from the video uploader\n",
    "                    if comment['authorChannelId']['value'] != uploader_channel_id:\n",
    "                        comments.append(comment['textDisplay'])\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "            # Print the first 5 comments\n",
    "            print(comments[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"No video found for the provided ID.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel to CsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                              Video Title  \\\n",
      "0      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "1      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "2      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "3      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "4      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "\n",
      "                                  Comment  \n",
      "0                              I love you  \n",
      "1  Please give me one iPhone😢😢😢😢!🥺🥺🥺🥺🥺🥺🥺🥺  \n",
      "2       ❤❤😂😂🎉😢😮😮😅😊😊❤❤❤😂😂😂🎉🎉😢😢😢😮😮😅😅😊😊❤❤😂😂😢  \n",
      "3        It&#39;s a lot it&#39;s all good  \n",
      "4         Can you buy a car for my dad ??  \n",
      "Warning: 'Video ID' column not found. We need this to fetch likes and dislikes.\n",
      "Dataframe columns: ['Channel Name', 'Video Title', 'Comment']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "# Get the channel name and upload playlist ID\n",
    "def get_channel_info(channel_id):\n",
    "    response = youtube.channels().list(\n",
    "        part='snippet,contentDetails',\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        channel = response['items'][0]['snippet']['title']\n",
    "        upload_playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        return channel, upload_playlist_id\n",
    "    return None, None\n",
    "\n",
    "# Fetch 100 video IDs from the uploads playlist\n",
    "def get_video_ids_from_channel(upload_playlist_id, max_results=100):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        response = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=upload_playlist_id,\n",
    "            maxResults=50,  # API allows max 50 results per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['snippet']['resourceId']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(video_ids) >= max_results:\n",
    "            break\n",
    "    \n",
    "    return video_ids[:max_results]\n",
    "\n",
    "# Fetch the video title\n",
    "def get_video_title(video_id):\n",
    "    response = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        return response['items'][0]['snippet']['title']\n",
    "    return None\n",
    "\n",
    "# Fetch comments for a video\n",
    "def get_comments(video_id, max_comments=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Fetch up to 100 comments per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(comments) >= max_comments:\n",
    "            break\n",
    "\n",
    "    return comments[:max_comments]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "def write_to_csv(channel_name, video_data):\n",
    "    with open(f'{channel_name}_video_comments.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Channel Name\", \"Video Title\", \"Comment\"])\n",
    "        \n",
    "        for video_title, comments in video_data.items():\n",
    "            for comment in comments:\n",
    "                writer.writerow([channel_name, video_title, comment])\n",
    "\n",
    "# Main Function\n",
    "def fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100):\n",
    "    # Get channel information\n",
    "    channel_name, upload_playlist_id = get_channel_info(channel_id)\n",
    "    \n",
    "    if not channel_name or not upload_playlist_id:\n",
    "        print(\"Invalid channel ID or channel information not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Fetching data for channel: {channel_name}\")\n",
    "\n",
    "    # Get video IDs\n",
    "    video_ids = get_video_ids_from_channel(upload_playlist_id, max_results=max_videos)\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # For each video, get the title and comments\n",
    "    for video_id in video_ids:\n",
    "        video_title = get_video_title(video_id)\n",
    "        if video_title:\n",
    "            print(f\"Fetching comments for video: {video_title}\")\n",
    "            comments = get_comments(video_id, max_comments=max_comments_per_video)\n",
    "            video_data[video_title] = comments\n",
    "\n",
    "    # Write all data to a CSV\n",
    "    write_to_csv(channel_name, video_data)\n",
    "    print(f\"Data saved to {channel_name}_video_comments.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_video_stats() got an unexpected keyword argument 'max_videos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Replace with the actual YouTube channel ID\u001b[39;00m\n\u001b[1;32m      4\u001b[0m channel_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUCX6OQ3DkcsbYNE6H8uQQuVA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mget_video_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchannel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_videos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_comments_per_video\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m start_time \u001b[38;5;241m-\u001b[39m end_time\n",
      "\u001b[0;31mTypeError\u001b[0m: get_video_stats() got an unexpected keyword argument 'max_videos'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Replace with the actual YouTube channel ID\n",
    "channel_id = f'UCX6OQ3DkcsbYNE6H8uQQuVA'\n",
    "get_video_stats(channel_id, max_videos=100, max_comments_per_video=100)\n",
    "end_time = time.time()\n",
    "execution_time = start_time - end_time\n",
    "print(\"Execution time:\",execution_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Processing for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"GPU is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                              Video Title  \\\n",
      "0      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "1      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "2      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "3      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "4      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "\n",
      "                                  Comment  \n",
      "0                              I love you  \n",
      "1  Please give me one iPhone😢😢😢😢!🥺🥺🥺🥺🥺🥺🥺🥺  \n",
      "2       ❤❤😂😂🎉😢😮😮😅😊😊❤❤❤😂😂😂🎉🎉😢😢😢😮😮😅😅😊😊❤❤😂😂😢  \n",
      "3        It&#39;s a lot it&#39;s all good  \n",
      "4         Can you buy a car for my dad ??  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Channel Name  9800 non-null   object\n",
      " 1   Video Title   9800 non-null   object\n",
      " 2   Comment       9800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 229.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Preprocess the data\n",
    "# Use the 'Comment' column as our input and 'Video Title' as our target\n",
    "X = df['Comment'].values\n",
    "y = df['Video Title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 processed comments with emojis:\n",
      "0                      i love you\n",
      "1       please give me one iphone\n",
      "2                                \n",
      "3          its a lot its all good\n",
      "4    can you buy a car for my dad\n",
      "Name: processed_comment_with_emojis, dtype: object\n",
      "                                  Comment  \\\n",
      "0                              I love you   \n",
      "1  Please give me one iPhone😢😢😢😢!🥺🥺🥺🥺🥺🥺🥺🥺   \n",
      "2       ❤❤😂😂🎉😢😮😮😅😊😊❤❤❤😂😂😂🎉🎉😢😢😢😮😮😅😅😊😊❤❤😂😂😢   \n",
      "3        It&#39;s a lot it&#39;s all good   \n",
      "4         Can you buy a car for my dad ??   \n",
      "\n",
      "                                Preprocessed_Comment  \n",
      "0                                         I love you  \n",
      "1  Please give me one iPhone:crying_face::crying_...  \n",
      "2  :red_heart::red_heart::face_with_tears_of_joy:...  \n",
      "3                   It&#39;s a lot it&#39;s all good  \n",
      "4                    Can you buy a car for my dad ??  \n",
      "First comment tensor:\n",
      "tensor([ 73,  32, 108, 111, 118, 101,  32, 121, 111, 117])\n",
      "Shape of the first tensor: torch.Size([10])\n",
      "Total number of tensors: 9800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from emoji import demojize\n",
    "\n",
    "\n",
    "# Update the preprocessing function to include emoji conversion\n",
    "def preprocess_text_with_emojis(text):\n",
    "    # Convert emojis to text\n",
    "    \n",
    "    emoji_pattern = re.compile(r':([a-zA-Z_]+):')\n",
    "    unique_emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # Reconstruct the text with only one instance of each emoji\n",
    "    for emote in unique_emojis:\n",
    "        text = re.sub(f'(:{emote}:)+', f':{emote}:', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Reprocess the comments with the updated function\n",
    "df['processed_comment_with_emojis'] = df['Comment'].apply(preprocess_text_with_emojis)\n",
    "\n",
    "print(\"\\\n",
    "First 5 processed comments with emojis:\")\n",
    "\n",
    "print(df['processed_comment_with_emojis'].head())\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Replace emojis with their meanings\n",
    "    return demojize(comment, language='en')\n",
    "\n",
    "# Preprocess the comments\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(df[['Comment', 'Preprocessed_Comment']].head())\n",
    "\n",
    "# Convert preprocessed comments to PyTorch tensors\n",
    "comment_tensors = [torch.tensor([ord(c) for c in comment]) for comment in df['Preprocessed_Comment']]\n",
    "\n",
    "# Display the first tensor\n",
    "print(\"\\\n",
    "First comment tensor:\")\n",
    "print(comment_tensors[0])\n",
    "\n",
    "# Print the shape of the first tensor\n",
    "print(\"\\\n",
    "Shape of the first tensor:\", comment_tensors[0].shape)\n",
    "\n",
    "# Print the total number of tensors\n",
    "print(\"\\\n",
    "Total number of tensors:\", len(comment_tensors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
