{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.errors import HttpError\n",
    "\n",
    "# # If modifying these scopes, delete the file token.json.\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\",\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "\n",
    "# creds = None\n",
    "# # The file token.json stores the user's access and refresh tokens, and is\n",
    "# # created automatically when the authorization flow completes for the first\n",
    "# # time.\n",
    "# if os.path.exists(\"token.json\"):\n",
    "#   creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# # If there are no (valid) credentials available, let the user log in.\n",
    "# if not creds or not creds.valid:\n",
    "#   if creds and creds.expired and creds.refresh_token:\n",
    "#     creds.refresh(Request())\n",
    "#   else:\n",
    "#     flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#         \"credentials.json\", SCOPES\n",
    "#     )\n",
    "#     creds = flow.run_local_server(port=0)\n",
    "#   # Save the credentials for the next run\n",
    "#   with open(\"token.json\", \"w\") as token:\n",
    "#     token.write(creds.to_json())\n",
    "\n",
    "# try:\n",
    "#   youtube = build(\"youtube\", \"v3\", credentials=creds)\n",
    "\n",
    "#       # Taking input from the user and slicing for video id\n",
    "#   video_id = input('Enter Youtube Video URL: ')[-11:]\n",
    "#   print(\"video id: \" + video_id)\n",
    "  \n",
    "#   # Getting the channelId of the video uploader\n",
    "#   video_response = youtube.videos().list(\n",
    "#       part='snippet',\n",
    "#       id=video_id\n",
    "#   ).execute()\n",
    "  \n",
    "#   print(video_response)\n",
    "#   # Splitting the response for channelID\n",
    "#   video_snippet = video_response['items'][0]['snippet']\n",
    "#   uploader_channel_id = video_snippet['channelId']\n",
    "#   print(\"channel id: \" + uploader_channel_id)\n",
    "  \n",
    "# except HttpError as err:\n",
    "#   print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video id: oWo9SNcyxlI\n",
      "Video Response: {'kind': 'youtube#videoListResponse', 'etag': 'FDRV--UbntAuYJOaIEc1BeeHFCo', 'items': [{'kind': 'youtube#video', 'etag': 'ppDsgJg80plQ9aIWXXT-RcV1qtM', 'id': 'oWo9SNcyxlI', 'snippet': {'publishedAt': '2022-07-27T13:15:20Z', 'channelId': 'UC8UmuMDlWB1egqH5nyy0pDw', 'title': 'Sentiment Analysis with LSTM | Deep Learning with Keras | Neural Networks | Project#8', 'description': 'We shall train three separate Neural Networks, namely: a Simple Neural Net, a Convolutional Neural Net and a Long Short Term Memory Neural Net. LSTM networks are actually considered to be quite suitable for handling NLP problems, and by the end of this video, you will understand why.\\n\\nSentiment Classification LSTM Model we shall be training as part of this tutorial is so good that it not only predicts the user movie review sentiment as positive/negative, it also does a fabulous job in predicting IMDb Rating itself corresponding to these reviews with mind blowing accuracy.\\n\\nüî• During the course of next ~25mins, we shall discuss:\\na. Word Embedding (which is a powerful feature extraction technique), \\nb. Intuition on various Neural Network approaches (Simple / CNN /LSTM), \\nc. Concept of Overfitting (that happens when a model memorises training data, rather than understanding general distribution) \\nd. And, of course, our high-level solution architecture for training our Sentiment Classification Models\\n\\nüî• Sections:\\n\\n00:00 Introduction\\n02:17 Solution Architecture\\n03:50 Word Embeddings, Neural Networks & Overfitting\\n08:56 Free Skillcate Project Toolkit\\n09:55 Code: Intro\\n11:42 Code: Data Preprocessing\\n13:44 Code: Tokenizer & Embedding Layer\\n15:50 Code: Simple NN Training\\n17:50 Code: CNN Training\\n19:27 Code: LSTM Training\\n21:03 Having fun with Model\\n22:59 Updates on Free 1:1 Mentoring\\n\\nüî• Important Links:\\n\\nToolkit link: https://drive.google.com/drive/folders/1TK9k41RT8Nf3IhzerNWHpEqWztsk2gAP?usp=sharing\\n\\nSentiment Analysis Project based Review Classification Project from Skillcate: https://youtu.be/zwR6M5zpnWs\\n\\nSentiment Analysis Project (End-to-end) with ML Model Building + Deployment (using Flask):\\n- Model Building: https://youtu.be/lKAdxN0qrgk (Part-1)\\n- Model Deployment: https://youtu.be/KEQCVwJU5KU (Part-2)\\n\\nGitHub Repo Link: https://github.com/skillcate/movie-sentiment-analysis-with-deep-neural-networks.git\\n\\n\\nüî• Do like, share & subscribe to our channel. Keep in touch:\\n\\nEmail: skillcate@gmail.com\\nWebsite: https://www.skillcate.com\\nFacebook: https://www.facebook.com/groups/mlprojects\\nTelegram: https://t.me/skillcate', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/hqdefault.jpg', 'width': 480, 'height': 360}, 'standard': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/sddefault.jpg', 'width': 640, 'height': 480}, 'maxres': {'url': 'https://i.ytimg.com/vi/oWo9SNcyxlI/maxresdefault.jpg', 'width': 1280, 'height': 720}}, 'channelTitle': 'Skillcate AI', 'tags': ['sentiment analysis', 'sentiment analysis project', 'sentiment analysis with deep learning', 'sentiment analysis lstm', 'sentiment analysis using machine learning', 'sentiment analysis using lstm', 'sentiment analysis with lstm', 'lstm application', 'sentiment analysis with keras', 'sentiment analysis deep learning', 'amazon alexa review classification', 'review classification', 'review classification using deep learning', 'deep learning project', 'lstm project', 'sentiment analysis neural network'], 'categoryId': '27', 'liveBroadcastContent': 'none', 'defaultLanguage': 'en-IN', 'localized': {'title': 'Sentiment Analysis with LSTM | Deep Learning with Keras | Neural Networks | Project#8', 'description': 'We shall train three separate Neural Networks, namely: a Simple Neural Net, a Convolutional Neural Net and a Long Short Term Memory Neural Net. LSTM networks are actually considered to be quite suitable for handling NLP problems, and by the end of this video, you will understand why.\\n\\nSentiment Classification LSTM Model we shall be training as part of this tutorial is so good that it not only predicts the user movie review sentiment as positive/negative, it also does a fabulous job in predicting IMDb Rating itself corresponding to these reviews with mind blowing accuracy.\\n\\nüî• During the course of next ~25mins, we shall discuss:\\na. Word Embedding (which is a powerful feature extraction technique), \\nb. Intuition on various Neural Network approaches (Simple / CNN /LSTM), \\nc. Concept of Overfitting (that happens when a model memorises training data, rather than understanding general distribution) \\nd. And, of course, our high-level solution architecture for training our Sentiment Classification Models\\n\\nüî• Sections:\\n\\n00:00 Introduction\\n02:17 Solution Architecture\\n03:50 Word Embeddings, Neural Networks & Overfitting\\n08:56 Free Skillcate Project Toolkit\\n09:55 Code: Intro\\n11:42 Code: Data Preprocessing\\n13:44 Code: Tokenizer & Embedding Layer\\n15:50 Code: Simple NN Training\\n17:50 Code: CNN Training\\n19:27 Code: LSTM Training\\n21:03 Having fun with Model\\n22:59 Updates on Free 1:1 Mentoring\\n\\nüî• Important Links:\\n\\nToolkit link: https://drive.google.com/drive/folders/1TK9k41RT8Nf3IhzerNWHpEqWztsk2gAP?usp=sharing\\n\\nSentiment Analysis Project based Review Classification Project from Skillcate: https://youtu.be/zwR6M5zpnWs\\n\\nSentiment Analysis Project (End-to-end) with ML Model Building + Deployment (using Flask):\\n- Model Building: https://youtu.be/lKAdxN0qrgk (Part-1)\\n- Model Deployment: https://youtu.be/KEQCVwJU5KU (Part-2)\\n\\nGitHub Repo Link: https://github.com/skillcate/movie-sentiment-analysis-with-deep-neural-networks.git\\n\\n\\nüî• Do like, share & subscribe to our channel. Keep in touch:\\n\\nEmail: skillcate@gmail.com\\nWebsite: https://www.skillcate.com\\nFacebook: https://www.facebook.com/groups/mlprojects\\nTelegram: https://t.me/skillcate'}, 'defaultAudioLanguage': 'en-IN'}}], 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}\n",
      "channel id: UC8UmuMDlWB1egqH5nyy0pDw\n",
      "Fetching Comments...\n",
      "['can we stack these 3 models on top of each other <br>if so how?', 'Bro, you missed the padding function(pad_sequences) to include in code....', 'Can we use same procedure for emotions instead of sentiments ....and for product reviews', 'Long live your hand on the explanation', 'can you tell how to add flask to it please']\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "API_KEY = ''  # Put in your API Key\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)  # initializing Youtube API\n",
    "\n",
    "# Improved input handling for video ID\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "video_url = input('Enter YouTube Video URL: ')\n",
    "video_id = extract_video_id(video_url)\n",
    "\n",
    "if not video_id:\n",
    "    print(\"Invalid YouTube URL\")\n",
    "else:\n",
    "    print(\"video id: \" + video_id)\n",
    "\n",
    "    # Getting the channelId of the video uploader\n",
    "    try:\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        print(\"Video Response:\", video_response)  # Debugging step\n",
    "\n",
    "        if 'items' in video_response and video_response['items']:\n",
    "            video_snippet = video_response['items'][0]['snippet']\n",
    "            uploader_channel_id = video_snippet.get('channelId', 'No channelId found')\n",
    "            print(\"channel id: \" + uploader_channel_id)\n",
    "\n",
    "            # Fetch comments\n",
    "            print(\"Fetching Comments...\")\n",
    "            comments = []\n",
    "            nextPageToken = None\n",
    "            while len(comments) < 600:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100,  # You can fetch up to 100 comments per request\n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    # Check if the comment is not from the video uploader\n",
    "                    if comment['authorChannelId']['value'] != uploader_channel_id:\n",
    "                        comments.append(comment['textDisplay'])\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "            # Print the first 5 comments\n",
    "            print(comments[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"No video found for the provided ID.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel to CsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "import time\n",
    "\n",
    "# Get the channel name and upload playlist ID\n",
    "def get_channel_info(channel_id):\n",
    "    response = youtube.channels().list(\n",
    "        part='snippet,contentDetails',\n",
    "        id=channel_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        channel = response['items'][0]['snippet']['title']\n",
    "        upload_playlist_id = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        return channel, upload_playlist_id\n",
    "    return None, None\n",
    "\n",
    "# Fetch 100 video IDs from the uploads playlist\n",
    "def get_video_ids_from_channel(upload_playlist_id, max_results=100):\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(video_ids) < max_results:\n",
    "        response = youtube.playlistItems().list(\n",
    "            part='snippet',\n",
    "            playlistId=upload_playlist_id,\n",
    "            maxResults=50,  # API allows max 50 results per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "        \n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['snippet']['resourceId']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(video_ids) >= max_results:\n",
    "            break\n",
    "    \n",
    "    return video_ids[:max_results]\n",
    "\n",
    "# Fetch the video title\n",
    "def get_video_title(video_id):\n",
    "    response = youtube.videos().list(\n",
    "        part='snippet',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "    if 'items' in response and len(response['items']) > 0:\n",
    "        return response['items'][0]['snippet']['title']\n",
    "    return None\n",
    "\n",
    "# Fetch comments for a video\n",
    "def get_comments(video_id, max_comments=100):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            maxResults=100,  # Fetch up to 100 comments per request\n",
    "            pageToken=next_page_token\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            comments.append(comment)\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token or len(comments) >= max_comments:\n",
    "            break\n",
    "\n",
    "    return comments[:max_comments]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "def write_to_csv(channel_name, video_data):\n",
    "    with open(f'{channel_name}_video_comments.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Channel Name\", \"Video Title\", \"Comment\"])\n",
    "        \n",
    "        for video_title, comments in video_data.items():\n",
    "            for comment in comments:\n",
    "                writer.writerow([channel_name, video_title, comment])\n",
    "\n",
    "# Main Function\n",
    "def fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100):\n",
    "    start_time = time.time()\n",
    "    # Get channel information\n",
    "    channel_name, upload_playlist_id = get_channel_info(channel_id)\n",
    "    \n",
    "    if not channel_name or not upload_playlist_id:\n",
    "        print(\"Invalid channel ID or channel information not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Fetching data for channel: {channel_name}\")\n",
    "\n",
    "    # Get video IDs\n",
    "    video_ids = get_video_ids_from_channel(upload_playlist_id, max_results=max_videos)\n",
    "\n",
    "    video_data = {}\n",
    "\n",
    "    # For each video, get the title and comments\n",
    "    for video_id in video_ids:\n",
    "        video_title = get_video_title(video_id)\n",
    "        if video_title:\n",
    "            print(f\"Fetching comments for video: {video_title}\")\n",
    "            comments = get_comments(video_id, max_comments=max_comments_per_video)\n",
    "            video_data[video_title] = comments\n",
    "    \n",
    "    \n",
    "    # Write all data to a CSV\n",
    "    write_to_csv(channel_name, video_data)\n",
    "    print(f\"Data saved to {channel_name}_video_comments.csv\")\n",
    "    end_time = time.time()\n",
    "    execution_time = start_time - end_time\n",
    "    print(\"Execution time:\",execution_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for channel: MrBeast\n",
      "Fetching comments for video: How Many Twins Can You Spot?\n",
      "Fetching comments for video: Spot The Difference, Win $10,000\n",
      "Fetching comments for video: 100 Identical Twins Fight For $250,000\n",
      "Fetching comments for video: Running With Bigger And Bigger Lunchlys\n",
      "Fetching comments for video: Holding Bigger And Bigger Dogs\n",
      "Fetching comments for video: Men Vs Women Survive The Wilderness For $500,000\n",
      "Fetching comments for video: Will A Guitar Boat Hold My Weight?\n",
      "Fetching comments for video: Real Or Cake For $10,000\n",
      "Fetching comments for video: 7 Days Stranded In A Cave\n",
      "Fetching comments for video: Running With Bigger And Bigger Feastables\n",
      "Fetching comments for video: Survive 100 Days In Nuclear Bunker, Win $500,000\n",
      "Fetching comments for video: Spot The Fake Animal For $10,000\n",
      "Fetching comments for video: 50 YouTubers Fight For $1,000,000\n",
      "Fetching comments for video: Pass The Phone To‚Ä¶\n",
      "Fetching comments for video: How Many Balloons Does It Take To Fly?\n",
      "Fetching comments for video: I Built 100 Houses And Gave Them Away!\n",
      "Fetching comments for video: World‚Äôs Deadliest Obstacle Course!\n",
      "Fetching comments for video: $10,000 Every Day You Survive In The Wilderness\n",
      "Fetching comments for video: Sprinting with More and More Money\n",
      "Fetching comments for video: Giving 1000 Phones Away\n",
      "Fetching comments for video: Bottle Head Smashing World Record Attempt!\n",
      "Fetching comments for video: Protect The Yacht, Keep It!\n",
      "Fetching comments for video: Ages 1-100 Try My Chocolate\n",
      "Fetching comments for video: Spot The Hidden People For $10,000\n",
      "Fetching comments for video: Would You Split Or Steal $250,000?\n",
      "Fetching comments for video: Buy Feastables, Win Unlimited Money\n",
      "Fetching comments for video: In 10 Minutes This Room Will Explode!\n",
      "Fetching comments for video: The World's Fastest Cleaners\n",
      "Fetching comments for video: Ages 1 - 100 Decide Who Wins $250,000\n",
      "Fetching comments for video: Guess The Gift, Keep It\n",
      "Fetching comments for video: I‚Äôm Giving My 250M Subscriber $25,000\n",
      "Fetching comments for video: Anything You Touch, You Keep!\n",
      "Fetching comments for video: 7 Days Stranded On An Island\n",
      "Fetching comments for video: Keep Track Of Car, Win $10,000\n",
      "Fetching comments for video: Protect The Lamborghini, Keep It!\n",
      "Fetching comments for video: I Survived 7 Days In An Abandoned City\n",
      "Fetching comments for video: Unboxing My 200M Subscriber Play Button\n",
      "Fetching comments for video: I Filled Chandler‚Äôs Car With Feastables\n",
      "Fetching comments for video: Buy Feastables, Win $10,000\n",
      "Fetching comments for video: Face Your Biggest Fear To Win $800,000\n",
      "Fetching comments for video: $1 vs $250,000,000 Private Island!\n",
      "Fetching comments for video: Protect $500,000 Keep It!\n",
      "Fetching comments for video: I Spent 7 Days In Solitary Confinement\n",
      "Fetching comments for video: I Saved 100 Dogs From Dying\n",
      "Fetching comments for video: Survive 100 Days Trapped, Win $500,000\n",
      "Fetching comments for video: Feeding A Dog $1 vs $10,000 Steak\n",
      "Fetching comments for video: Could You Walk Up A Skyscraper?\n",
      "Fetching comments for video: $10,000 Every Day You Survive In A Grocery Store\n",
      "Fetching comments for video: $1 vs $10,000,000 Job!\n",
      "Fetching comments for video: I Spent 7 Days Buried Alive\n",
      "Fetching comments for video: I Gave Away A House On Halloween\n",
      "Fetching comments for video: Giving Car Keys Instead Of Candy On Halloween\n",
      "Fetching comments for video: I Built 100 Wells In Africa\n",
      "Fetching comments for video: Furthest Away From Me Wins $10,000\n",
      "Fetching comments for video: World‚Äôs Most Expensive Bed\n",
      "Fetching comments for video: World‚Äôs Deadliest Laser Maze!\n",
      "Fetching comments for video: World‚Äôs Most Expensive Coffee\n",
      "Fetching comments for video: $100,000,000 Bathroom\n",
      "Fetching comments for video: Miranda Cosgrove Said What?\n",
      "Fetching comments for video: $1 vs $100,000,000 House!\n",
      "Fetching comments for video: I Tipped A Pizza Delivery Driver A Car\n",
      "Fetching comments for video: World's Most Dangerous Trap!\n",
      "Fetching comments for video: I NEED 1 MORE ùó¶ùó®ùóïùó¶ùóñùó•ùóúùóïùóòùó•\n",
      "Fetching comments for video: Guess The Gift, Keep It\n",
      "Fetching comments for video: Spot The Hidden People For $10,000\n",
      "Fetching comments for video: I Paid A Random Student‚Äôs College Tuition\n",
      "Fetching comments for video: $100,000,000 Car Doors\n",
      "Fetching comments for video: $1 vs $100,000,000 Car!\n",
      "Fetching comments for video: Extreme Home Makeover!\n",
      "Fetching comments for video: Katana Vs Bullet\n",
      "Fetching comments for video: How Many School Buses Can We Stack?\n",
      "Fetching comments for video: Lamborghini Vs World's Largest Shredder\n",
      "Fetching comments for video: Feeding A Cat $10 Vs $10,000 Sushi\n",
      "Fetching comments for video: Make This Kick, Win Super Bowl Tickets\n",
      "Fetching comments for video: Metal Pipe Vs School Bus\n",
      "Fetching comments for video: Every Country On Earth Fights For $250,000!\n",
      "Fetching comments for video: Can You Beat A Girl In Arm Wrestling?\n",
      "Fetching comments for video: $1 vs $250,000 Vacation!\n",
      "Fetching comments for video: I Ate The World‚Äôs Most Poisonous Fish\n",
      "Fetching comments for video: 7 Days Stranded At Sea\n",
      "Fetching comments for video: I Traded My Car At a Red Light\n",
      "Fetching comments for video: Train Vs Giant Pit\n",
      "Fetching comments for video: I Buried Treasure in the Bermuda Triangle\n",
      "Fetching comments for video: $1 vs $1,000,000,000 Yacht!\n",
      "Fetching comments for video: Do Pawnshops Scam You?\n",
      "Fetching comments for video: Ages 1 - 100 Fight For $500,000\n",
      "Fetching comments for video: I Got Naruto to Subscribe to Me\n",
      "Fetching comments for video: I Made An Egg Sandwich With @BayashiTV_\n",
      "Fetching comments for video: 1,000 Deaf People Hear For The First Time\n",
      "Fetching comments for video: Would You Pet a Cheetah in Africa?\n",
      "Fetching comments for video: Tipping A Waitress A Car\n",
      "Fetching comments for video: Would you go on a Blind Date in Italy?\n",
      "Fetching comments for video: $1 vs $500,000 Plane Ticket!\n",
      "Fetching comments for video: I Sent a Subscriber to Disneyland\n",
      "Fetching comments for video: I Paid A Real Assassin To Try To Kill Me\n",
      "Fetching comments for video: Do Men Lie About Their Height?\n",
      "Fetching comments for video: 1,000 Blind People See For The First Time\n",
      "Fetching comments for video: I Survived 50 Hours In Antarctica\n",
      "Fetching comments for video: Hydraulic Press Vs Lamborghini\n",
      "Fetching comments for video: Would You Fly To Paris For A Baguette?\n",
      "Data saved to MrBeast_video_comments.csv\n",
      "Execution time: -26.033523082733154\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Replace with the actual YouTube channel ID\n",
    "channel_id = f'UCX6OQ3DkcsbYNE6H8uQQuVA'\n",
    "fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Processing for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"GPU is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                              Video Title  \\\n",
      "0      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "1      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "2      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "3      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "4      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "\n",
      "                                  Comment  \n",
      "0                              I love you  \n",
      "1  Please give me one iPhoneüò¢üò¢üò¢üò¢!ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫  \n",
      "2       ‚ù§‚ù§üòÇüòÇüéâüò¢üòÆüòÆüòÖüòäüòä‚ù§‚ù§‚ù§üòÇüòÇüòÇüéâüéâüò¢üò¢üò¢üòÆüòÆüòÖüòÖüòäüòä‚ù§‚ù§üòÇüòÇüò¢  \n",
      "3        It&#39;s a lot it&#39;s all good  \n",
      "4         Can you buy a car for my dad ??  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Channel Name  9800 non-null   object\n",
      " 1   Video Title   9800 non-null   object\n",
      " 2   Comment       9800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 229.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Preprocess the data\n",
    "# Use the 'Comment' column as our input and 'Video Title' as our target\n",
    "X = df['Comment'].values\n",
    "y = df['Video Title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 processed comments with emojis:\n",
      "0                      i love you\n",
      "1       please give me one iphone\n",
      "2                                \n",
      "3          its a lot its all good\n",
      "4    can you buy a car for my dad\n",
      "Name: processed_comment_with_emojis, dtype: object\n",
      "                                  Comment  \\\n",
      "0                              I love you   \n",
      "1  Please give me one iPhoneüò¢üò¢üò¢üò¢!ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫   \n",
      "2       ‚ù§‚ù§üòÇüòÇüéâüò¢üòÆüòÆüòÖüòäüòä‚ù§‚ù§‚ù§üòÇüòÇüòÇüéâüéâüò¢üò¢üò¢üòÆüòÆüòÖüòÖüòäüòä‚ù§‚ù§üòÇüòÇüò¢   \n",
      "3        It&#39;s a lot it&#39;s all good   \n",
      "4         Can you buy a car for my dad ??   \n",
      "\n",
      "                                Preprocessed_Comment  \n",
      "0                                         I love you  \n",
      "1  Please give me one iPhone:crying_face::crying_...  \n",
      "2  :red_heart::red_heart::face_with_tears_of_joy:...  \n",
      "3                   It&#39;s a lot it&#39;s all good  \n",
      "4                    Can you buy a car for my dad ??  \n",
      "First comment tensor:\n",
      "tensor([ 73,  32, 108, 111, 118, 101,  32, 121, 111, 117])\n",
      "Shape of the first tensor: torch.Size([10])\n",
      "Total number of tensors: 9800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from emoji import demojize\n",
    "\n",
    "\n",
    "# Update the preprocessing function to include emoji conversion\n",
    "def preprocess_text_with_emojis(text):\n",
    "    # Convert emojis to text\n",
    "    \n",
    "    emoji_pattern = re.compile(r':([a-zA-Z_]+):')\n",
    "    unique_emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # Reconstruct the text with only one instance of each emoji\n",
    "    for emote in unique_emojis:\n",
    "        text = re.sub(f'(:{emote}:)+', f':{emote}:', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Reprocess the comments with the updated function\n",
    "df['processed_comment_with_emojis'] = df['Comment'].apply(preprocess_text_with_emojis)\n",
    "\n",
    "print(\"\\\n",
    "First 5 processed comments with emojis:\")\n",
    "\n",
    "print(df['processed_comment_with_emojis'].head())\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Replace emojis with their meanings\n",
    "    return demojize(comment, language='en')\n",
    "\n",
    "# Preprocess the comments\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(df[['Comment', 'Preprocessed_Comment']].head())\n",
    "\n",
    "# Convert preprocessed comments to PyTorch tensors\n",
    "comment_tensors = [torch.tensor([ord(c) for c in comment]) for comment in df['Preprocessed_Comment']]\n",
    "\n",
    "# Display the first tensor\n",
    "print(\"\\\n",
    "First comment tensor:\")\n",
    "print(comment_tensors[0])\n",
    "\n",
    "# Print the shape of the first tensor\n",
    "print(\"\\\n",
    "Shape of the first tensor:\", comment_tensors[0].shape)\n",
    "\n",
    "# Print the total number of tensors\n",
    "print(\"\\\n",
    "Total number of tensors:\", len(comment_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Comment Sentiment\n",
      "0                              I love you  Positive\n",
      "1  Please give me one iPhoneüò¢üò¢üò¢üò¢!ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫ü•∫  Negative\n",
      "2       ‚ù§‚ù§üòÇüòÇüéâüò¢üòÆüòÆüòÖüòäüòä‚ù§‚ù§‚ù§üòÇüòÇüòÇüéâüéâüò¢üò¢üò¢üòÆüòÆüòÖüòÖüòäüòä‚ù§‚ù§üòÇüòÇüò¢   Neutral\n",
      "3        It&#39;s a lot it&#39;s all good  Positive\n",
      "4         Can you buy a car for my dad ??   Neutral\n",
      "5                        üòÖüòÇüòÇüòÇüòÇüòÇüòÇüòÇüòÆüòÇüòÆüòÆüòÇüòÆüòÇüòÇ   Neutral\n",
      "6                                      üòÇ‚ù§  Positive\n",
      "7                           ‚ù§‚ù§‚ù§‚ù§ nice üëçüëçüëç  Positive\n",
      "8                                   üòÇüòÇüòÇüòÇüòÇ   Neutral\n",
      "9                                      ‚ù§‚ù§  Positive\n",
      "Sentiment Distribution:\n",
      "Sentiment\n",
      "Neutral     0.737245\n",
      "Positive    0.223980\n",
      "Negative    0.038776\n",
      "Name: proportion, dtype: float64\n",
      "Labeled data saved to 'MrBeast_video_comments_labeled.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from emoji import demojize\n",
    "\n",
    "def simple_sentiment_analysis(text):\n",
    "    text = demojize(text.lower())\n",
    "    \n",
    "    positive_words = ['love', 'great', 'awesome', 'amazing', 'good', 'best', 'like', 'thanks', 'thank you', ':smiling_face:', ':red_heart:']\n",
    "    negative_words = ['hate', 'bad', 'terrible', 'worst', 'dislike', 'awful', ':angry_face:', ':crying_face:']\n",
    "    \n",
    "    positive_count = sum(1 for word in positive_words if word in text)\n",
    "    negative_count = sum(1 for word in negative_words if word in text)\n",
    "    \n",
    "    if positive_count > negative_count:\n",
    "        return 'Positive'\n",
    "    elif negative_count > positive_count:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Apply sentiment analysis to all comments\n",
    "df['Sentiment'] = df['Comment'].apply(simple_sentiment_analysis)\n",
    "\n",
    "# Display the first few rows with sentiments\n",
    "print(df[['Comment', 'Sentiment']].head(10))\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\\n",
    "Sentiment Distribution:\")\n",
    "print(df['Sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Save the labeled data\n",
    "df.to_csv('MrBeast_video_comments_labeled.csv', index=False)\n",
    "print(\"\\\n",
    "Labeled data saved to 'MrBeast_video_comments_labeled.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
