{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os.path\n",
    "\n",
    "# from google.auth.transport.requests import Request\n",
    "# from google.oauth2.credentials import Credentials\n",
    "# from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# from googleapiclient.discovery import build\n",
    "# from googleapiclient.errors import HttpError\n",
    "\n",
    "# # If modifying these scopes, delete the file token.json.\n",
    "# SCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\",\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "\n",
    "# creds = None\n",
    "# # The file token.json stores the user's access and refresh tokens, and is\n",
    "# # created automatically when the authorization flow completes for the first\n",
    "# # time.\n",
    "# if os.path.exists(\"token.json\"):\n",
    "#   creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# # If there are no (valid) credentials available, let the user log in.\n",
    "# if not creds or not creds.valid:\n",
    "#   if creds and creds.expired and creds.refresh_token:\n",
    "#     creds.refresh(Request())\n",
    "#   else:\n",
    "#     flow = InstalledAppFlow.from_client_secrets_file(\n",
    "#         \"credentials.json\", SCOPES\n",
    "#     )\n",
    "#     creds = flow.run_local_server(port=0)\n",
    "#   # Save the credentials for the next run\n",
    "#   with open(\"token.json\", \"w\") as token:\n",
    "#     token.write(creds.to_json())\n",
    "\n",
    "# try:\n",
    "#   youtube = build(\"youtube\", \"v3\", credentials=creds)\n",
    "\n",
    "#       # Taking input from the user and slicing for video id\n",
    "#   video_id = input('Enter Youtube Video URL: ')[-11:]\n",
    "#   print(\"video id: \" + video_id)\n",
    "  \n",
    "#   # Getting the channelId of the video uploader\n",
    "#   video_response = youtube.videos().list(\n",
    "#       part='snippet',\n",
    "#       id=video_id\n",
    "#   ).execute()\n",
    "  \n",
    "#   print(video_response)\n",
    "#   # Splitting the response for channelID\n",
    "#   video_snippet = video_response['items'][0]['snippet']\n",
    "#   uploader_channel_id = video_snippet['channelId']\n",
    "#   print(\"channel id: \" + uploader_channel_id)\n",
    "  \n",
    "# except HttpError as err:\n",
    "#   print(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import re\n",
    "\n",
    "API_KEY = ''  # Put in your API Key\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)  # initializing Youtube API\n",
    "\n",
    "# Improved input handling for video ID\n",
    "def extract_video_id(url):\n",
    "    match = re.search(r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*', url)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "video_url = input('Enter YouTube Video URL: ')\n",
    "video_id = extract_video_id(video_url)\n",
    "\n",
    "if not video_id:\n",
    "    print(\"Invalid YouTube URL\")\n",
    "else:\n",
    "    print(\"video id: \" + video_id)\n",
    "\n",
    "    # Getting the channelId of the video uploader\n",
    "    try:\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "\n",
    "        print(\"Video Response:\", video_response)  # Debugging step\n",
    "\n",
    "        if 'items' in video_response and video_response['items']:\n",
    "            video_snippet = video_response['items'][0]['snippet']\n",
    "            uploader_channel_id = video_snippet.get('channelId', 'No channelId found')\n",
    "            print(\"channel id: \" + uploader_channel_id)\n",
    "\n",
    "            # Fetch comments\n",
    "            print(\"Fetching Comments...\")\n",
    "            comments = []\n",
    "            nextPageToken = None\n",
    "            while len(comments) < 600:\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part='snippet',\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100,  # You can fetch up to 100 comments per request\n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "                for item in response['items']:\n",
    "                    comment = item['snippet']['topLevelComment']['snippet']\n",
    "                    # Check if the comment is not from the video uploader\n",
    "                    if comment['authorChannelId']['value'] != uploader_channel_id:\n",
    "                        comments.append(comment['textDisplay'])\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "            # Print the first 5 comments\n",
    "            print(comments[:5])\n",
    "\n",
    "        else:\n",
    "            print(\"No video found for the provided ID.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel to CsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "from emoji import demojize\n",
    "import os\n",
    "\n",
    "# Set up the YouTube API client\n",
    "api_key = os.getenv('YOUTUBE_API_KEY')  # Make sure to set this environment variable\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "def get_video_stats(video_id):\n",
    "    try:\n",
    "        response = youtube.videos().list(\n",
    "            part='statistics',\n",
    "            id=video_id\n",
    "        ).execute()\n",
    "        \n",
    "        if 'items' in response and response['items']:\n",
    "            stats = response['items'][0]['statistics']\n",
    "            return {\n",
    "                'likes': int(stats.get('likeCount', 0)),\n",
    "                'dislikes': int(stats.get('dislikeCount', 0))  # Note: YouTube API no longer provides dislike counts\n",
    "            }\n",
    "        else:\n",
    "            return {'likes': 0, 'dislikes': 0}\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stats for video {video_id}: {str(e)}\")\n",
    "        return {'likes': 0, 'dislikes': 0}\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows to see the structure\n",
    "print(df.head())\n",
    "\n",
    "# Check if we have a 'Video ID' column\n",
    "if 'Video ID' not in df.columns:\n",
    "    print(\"Warning: 'Video ID' column not found. We need this to fetch likes and dislikes.\")\n",
    "else:\n",
    "    # Get unique video IDs\n",
    "    unique_video_ids = df['Video ID'].unique()\n",
    "    print(f\"Number of unique videos: {len(unique_video_ids)}\")\n",
    "    print(\"First few video IDs:\", unique_video_ids[:5])\n",
    "\n",
    "print(\"\\\n",
    "Dataframe columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# Replace with the actual YouTube channel ID\n",
    "channel_id = f'UCX6OQ3DkcsbYNE6H8uQQuVA'\n",
    "fetch_videos_and_comments(channel_id, max_videos=100, max_comments_per_video=100)\n",
    "end_time = time.time()\n",
    "execution_time = start_time - end_time\n",
    "print(\"Execution time:\",execution_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "# if is_cuda:\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"GPU is available\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Channel Name                              Video Title  \\\n",
      "0      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "1      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "2      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "3      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "4      MrBeast  Running With Bigger And Bigger Lunchlys   \n",
      "\n",
      "                                  Comment  \n",
      "0                              I love you  \n",
      "1  Please give me one iPhone😢😢😢😢!🥺🥺🥺🥺🥺🥺🥺🥺  \n",
      "2       ❤❤😂😂🎉😢😮😮😅😊😊❤❤❤😂😂😂🎉🎉😢😢😢😮😮😅😅😊😊❤❤😂😂😢  \n",
      "3        It&#39;s a lot it&#39;s all good  \n",
      "4         Can you buy a car for my dad ??  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9800 entries, 0 to 9799\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Channel Name  9800 non-null   object\n",
      " 1   Video Title   9800 non-null   object\n",
      " 2   Comment       9800 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 229.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('MrBeast_video_comments.csv', encoding='utf-8')\n",
    "\n",
    "# Display the first few rows and basic information about the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# Preprocess the data\n",
    "# Use the 'Comment' column as our input and 'Video Title' as our target\n",
    "X = df['Comment'].values\n",
    "y = df['Video Title'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 processed comments with emojis:\n",
      "0                      i love you\n",
      "1       please give me one iphone\n",
      "2                                \n",
      "3          its a lot its all good\n",
      "4    can you buy a car for my dad\n",
      "Name: processed_comment_with_emojis, dtype: object\n",
      "                                  Comment  \\\n",
      "0                              I love you   \n",
      "1  Please give me one iPhone😢😢😢😢!🥺🥺🥺🥺🥺🥺🥺🥺   \n",
      "2       ❤❤😂😂🎉😢😮😮😅😊😊❤❤❤😂😂😂🎉🎉😢😢😢😮😮😅😅😊😊❤❤😂😂😢   \n",
      "3        It&#39;s a lot it&#39;s all good   \n",
      "4         Can you buy a car for my dad ??   \n",
      "\n",
      "                                Preprocessed_Comment  \n",
      "0                                         I love you  \n",
      "1  Please give me one iPhone:crying_face::crying_...  \n",
      "2  :red_heart::red_heart::face_with_tears_of_joy:...  \n",
      "3                   It&#39;s a lot it&#39;s all good  \n",
      "4                    Can you buy a car for my dad ??  \n",
      "First comment tensor:\n",
      "tensor([ 73,  32, 108, 111, 118, 101,  32, 121, 111, 117])\n",
      "Shape of the first tensor: torch.Size([10])\n",
      "Total number of tensors: 9800\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from emoji import demojize\n",
    "\n",
    "\n",
    "# Update the preprocessing function to include emoji conversion\n",
    "def preprocess_text_with_emojis(text):\n",
    "    # Convert emojis to text\n",
    "    \n",
    "    emoji_pattern = re.compile(r':([a-zA-Z_]+):')\n",
    "    unique_emojis = set(emoji_pattern.findall(text))\n",
    "    \n",
    "    # Reconstruct the text with only one instance of each emoji\n",
    "    for emote in unique_emojis:\n",
    "        text = re.sub(f'(:{emote}:)+', f':{emote}:', text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters, numbers, and extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Reprocess the comments with the updated function\n",
    "df['processed_comment_with_emojis'] = df['Comment'].apply(preprocess_text_with_emojis)\n",
    "\n",
    "print(\"\\\n",
    "First 5 processed comments with emojis:\")\n",
    "\n",
    "print(df['processed_comment_with_emojis'].head())\n",
    "\n",
    "def preprocess_comment(comment):\n",
    "    # Replace emojis with their meanings\n",
    "    return demojize(comment, language='en')\n",
    "\n",
    "# Preprocess the comments\n",
    "df['Preprocessed_Comment'] = df['Comment'].apply(preprocess_comment)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "print(df[['Comment', 'Preprocessed_Comment']].head())\n",
    "\n",
    "# Convert preprocessed comments to PyTorch tensors\n",
    "comment_tensors = [torch.tensor([ord(c) for c in comment]) for comment in df['Preprocessed_Comment']]\n",
    "\n",
    "# Display the first tensor\n",
    "print(\"\\\n",
    "First comment tensor:\")\n",
    "print(comment_tensors[0])\n",
    "\n",
    "# Print the shape of the first tensor\n",
    "print(\"\\\n",
    "Shape of the first tensor:\", comment_tensors[0].shape)\n",
    "\n",
    "# Print the total number of tensors\n",
    "print(\"\\\n",
    "Total number of tensors:\", len(comment_tensors))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
